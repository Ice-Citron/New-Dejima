# AI Video Creator — Extension Plan
*New Dejima / Project Altiera — HackEurope Paris 2026*

---

## Context

The agent already autonomously builds and ships Android apps. The next vertical is **autonomous video content creation and distribution** — the agent generates, narrates, renders, and publishes videos with zero human input.

This directly amplifies the economic thesis: videos drive app installs, app installs drive API subscription revenue, YouTube monetization adds a fourth revenue stream on top of the existing three (ads, subscriptions, AI upsells).

---

## What We're Building

A new OpenClaw skill — **`ai-video-creator`** — that takes a topic or brief and:

1. Writes a full script + scene breakdown (Claude Opus 4.6)
2. Generates professional voice narration (ElevenLabs API)
3. Generates video scenes from text prompts (Wan 2.1 / Nano Banan 3)
4. Merges audio + video, adds captions (ffmpeg)
5. Runs a quality gate (duration check, sync check, corruption check)
6. Uploads autonomously to YouTube, TikTok, Instagram Reels
7. Tracks cost vs. revenue through Paid.ai

---

## Pipeline — 7 Phases (mirrors android-app-builder)

```
Phase 1: Script Generation
  └─ Input: topic/brief
  └─ Model: Claude Opus 4.6
  └─ Output: script.md (narration text + scene descriptions + metadata)

Phase 2: Voice Generation
  └─ API: ElevenLabs /v1/text-to-speech
  └─ Voice: configurable (default: "Rachel" or custom clone)
  └─ Output: narration.mp3 (with timestamps per segment)
  └─ GATE: audio duration > 0s, file exists, no API error

Phase 3: Video Generation
  └─ API: Wan 2.1 / Nano Banan 3 (text-to-video)
  └─ Input: scene descriptions from script.md
  └─ Parallel: generate N scene clips simultaneously
  └─ Output: scene_001.mp4, scene_002.mp4 ... scene_N.mp4
  └─ GATE: all clips generated, no corruption, min resolution check

Phase 4: Post-Production (ffmpeg)
  └─ Concatenate scene clips in order
  └─ Layer narration audio over video
  └─ Burn subtitles (auto-generated from script)
  └─ Add intro/outro (optional template)
  └─ Output: final_video.mp4

Phase 5: Quality Gate [HARD GATE — abort if fail]
  └─ Duration match: video length ≈ narration length (±10%)
  └─ File integrity: ffprobe validates no corruption
  └─ Resolution: min 1080p
  └─ Audio sync: verify narration audible
  └─ Retry: re-render if gate fails (max 3 attempts)

Phase 6: Upload & Distribution
  └─ YouTube: upload via YouTube Data API v3
      ├─ Title, description, tags (generated by Claude)
      ├─ Thumbnail (auto-generated or from scene frame)
      └─ Category, visibility (public/unlisted configurable)
  └─ TikTok: TikTok Content Posting API
  └─ Instagram Reels: Instagram Graph API
  └─ Twitter/X: upload as video tweet (optional)

Phase 7: Analytics & Cost Tracking
  └─ Paid.ai: log total token cost + API costs
  └─ Record: video_id, platform, upload_time, estimated_cpm
  └─ Agent wallet: update expected revenue timeline
  └─ Notify: log success + links to published videos
```

---

## APIs Required

| API | Purpose | Key Name | Status |
|-----|---------|----------|--------|
| ElevenLabs | Voice narration | `ELEVENLABS_API_KEY` | Already a stretch goal |
| Wan 2.1 / Nano Banan 3 | Video scene generation | `WAN_API_KEY` or `BANAN_API_KEY` | Need to add |
| YouTube Data API v3 | Upload + manage videos | `YOUTUBE_API_KEY` + OAuth 2.0 | Need OAuth flow |
| TikTok Content API | TikTok upload | `TIKTOK_CLIENT_KEY` + `TIKTOK_CLIENT_SECRET` | Need to add |
| Instagram Graph API | Reels upload | `INSTAGRAM_ACCESS_TOKEN` | Need to add |
| ffmpeg | A/V merging, subtitles | — (binary, already likely installed) | Check install |

> **Note on Nano Banan 3 / Wan 2.1**: Clarify the exact API endpoint and model name. If Wan 2.1 is self-hosted or API-based, confirm auth method. Banana.dev also works as an inference hosting layer for video models if needed.

---

## New Files to Create

```
config/openclaw/skills/ai-video-creator/
├── SKILL.md                    ← Full 7-phase pipeline instructions (main skill file)
├── prompts/
│   ├── script-writer.md        ← System prompt for Claude script generation
│   ├── scene-describer.md      ← Prompt to convert script segments → video prompts
│   └── metadata-generator.md  ← Prompt to generate YouTube title/desc/tags
└── templates/
    ├── subtitle.srt.template   ← SRT subtitle template
    └── video-config.json       ← Default resolution, duration, style settings
```

---

## .env Keys to Add

```bash
# AI Video Creator
ELEVENLABS_API_KEY=
ELEVENLABS_VOICE_ID=         # Default voice (e.g., Rachel = 21m00Tcm4TlvDq8ikWAM)
WAN_API_KEY=                 # Wan 2.1 / Nano Banan 3 API key
WAN_API_BASE_URL=            # Video generation endpoint

# Social Distribution
YOUTUBE_CLIENT_ID=
YOUTUBE_CLIENT_SECRET=
YOUTUBE_REFRESH_TOKEN=       # Pre-authorized OAuth refresh token
TIKTOK_CLIENT_KEY=
TIKTOK_CLIENT_SECRET=
TIKTOK_ACCESS_TOKEN=
INSTAGRAM_ACCESS_TOKEN=

# Video settings
DEFAULT_VIDEO_RESOLUTION=1920x1080
DEFAULT_VIDEO_DURATION_MAX=180   # seconds (3 min default)
VIDEO_OUTPUT_DIR=/tmp/dejima-videos
```

---

## Revenue Model

This skill unlocks a **fourth revenue stream**:

| Source | Mechanism | Timeline |
|--------|-----------|----------|
| YouTube Partner Program | CPM on views (~$2-5/1000 views) | After 1K subs + 4K watch hours |
| TikTok Creator Fund | Per-view payouts | After 10K followers |
| Affiliate links in descriptions | App store links → installs → IAP revenue | Immediate |
| Sponsored content (future) | Agent negotiates brand deals | Long-term |

**Cost per video:**
- Claude script generation: ~$0.05
- ElevenLabs TTS (avg 3 min): ~$0.10-0.20
- Wan 2.1 video generation (N scenes): ~$0.50-2.00 (TBD based on API pricing)
- Total estimated: **$0.65-2.25 per video**

At 100 videos/day, break-even is ~500 YouTube views per video (well within reach for tech/finance content).

---

## Integration with Existing System

### Connects to android-app-builder skill:
- Agent can **auto-generate a video** announcing each new app it builds
- App trailer: screen recordings + voice narration + upload = organic marketing
- Creates a **feedback loop**: apps → videos → installs → more revenue → more apps

### Connects to Paid.ai cost tracking:
- Every API call (ElevenLabs, Wan, YouTube) tracked as cost
- YouTube revenue signals fed back as revenue
- Agent's reproduction decision includes video ROI

### Connects to agent identity (C-3PO):
- Agent has a "content creator" persona layer for video scripts
- Can generate YouTube Shorts, long-form explainers, or app demos
- Voice persona consistent across all videos

---

## Implementation Order

### Step 1 — Foundation (do first)
- [ ] Confirm Nano Banan 3 / Wan 2.1 API access + test a single video generation call
- [ ] Confirm ElevenLabs key works + test TTS call
- [ ] Verify ffmpeg installed (`ffmpeg -version`)
- [ ] Create `ai-video-creator/SKILL.md` with Phases 1-5 (local pipeline only)

### Step 2 — Distribution
- [ ] Set up YouTube OAuth flow (one-time manual authorization → save refresh token to .env)
- [ ] Test YouTube upload with a dummy video
- [ ] Add TikTok API credentials + test upload
- [ ] Add Instagram Graph API + test Reels upload

### Step 3 — Full Autonomy
- [ ] Wire skill into OpenClaw agent (exec approvals for ffmpeg, curl, etc.)
- [ ] Test full end-to-end: prompt → published video
- [ ] Add to Paid.ai cost tracking
- [ ] Connect to android-app-builder: auto-generate app trailer on each build

### Step 4 — Optimization
- [ ] Parallel scene generation (generate all scenes simultaneously)
- [ ] Retry logic for failed video gen (same as compilation retry in android pipeline)
- [ ] Thumbnail generation (extract best frame or generate with image model)
- [ ] A/B test different voices and video styles

---

## Key Questions to Resolve Before Building

1. **Nano Banan 3 exact API**: Is this Wan 2.1 (Alibaba), banana.dev as an inference host, or a different model? Need API docs + endpoint URL.
2. **YouTube OAuth**: Needs one-time manual browser auth to get refresh token — plan for this during setup.
3. **Video length**: Short-form (60s Reels/TikTok) vs long-form (5-10 min YouTube)? Can do both with different configs.
4. **Content strategy**: Tech tutorials, app demos, finance/crypto explainers, or all of the above?
5. **Video style**: Pure narration over generated visuals, or screen recording of apps + narration?

---

## Exec Approvals to Add

```json
// In exec-approvals.json
"ffmpeg": ["*"],
"ffprobe": ["*"],
"curl -X POST https://api.elevenlabs.io/*": ["*"],
"curl -X POST *wan*": ["*"]
```

---

*This skill makes the agent a complete autonomous content business: it builds the product AND markets it.*
